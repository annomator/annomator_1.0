{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seteup for Annotator, Box Trainer and Mask Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick Install - Terminal\n",
    "pip install -r annotator_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quick Install - Jupyter\n",
    "\n",
    "### It is recommended to use Python 3.6.6 \n",
    "# Works with Python 2.7 and 3.3-3.6 (Not 3.7)\n",
    "### It is recommended to use virtualenv\n",
    "# Works as system/native, conda, virtualenv or pipenv\n",
    "### It is recommended to use Tensorflow 1.5.0 and TF Object Detection 1.0 BEFORE September 2018\n",
    "# Works with 1.5.0 - 1.10 (Not 1.11)\n",
    "\n",
    "# This is not best practice but this is how I did it:\n",
    "# Download Python 3.6 and update to pip 18\n",
    "# Create/activate the virtual environment\n",
    "! pip install jupyter==6.4 # latest (7.0.1 bug is still active)\n",
    "! pip install pillow==5.0.0 # some OS issues still exist with later versions\n",
    "! pip install matplotlib # latest and only - I will lock down a version if needed\n",
    "! pip install tensorflow==1.5.0 # or ! pip install tensorflow-gpu==1.5.0\n",
    "# I have later added this to ensure both 1.5.0 and 1.10.0 will work for most\n",
    "! pip install protobuf==3.6.1 # latest ok - simply to ensure future version control\n",
    "\n",
    "annomator_requirements.txt and annomator_requirements_gpu.txt are the result\n",
    "\n",
    "Windows - You may get an error msvcp140.dll\n",
    "Download the Visual C++ 2015 Redistributable Update 3\n",
    "\n",
    "# Download annomator from github\n",
    "# link\n",
    "\n",
    "# Annotating is setup for MSCOCO - see  Demo\n",
    "\n",
    "# I recommend using the following open source programs\n",
    "# Gimp to annotate masks\n",
    "# xnview for batching images\n",
    "# VLC to turn video into images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to train boxes or train masks you may need to protoc and PYTHONPATH\n",
    "# This is also needed for pbtxt, and therefore other models\n",
    "# - I have included a coco category_index for the annotator\n",
    "# - You can do the same for other models or your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protoc\n",
    "# If Windows and protobuf 3.6.1 you are already done installing - skip to PYTHONPATH\n",
    "\n",
    "# If you are using Windows then you can skip the protoc as it has already been done\n",
    "# Use the link below if you have issues\n",
    "# Only because it has been such an issue - annomator is operating system agnostic \n",
    "# The linux and mac install is much simpler as since 3.5+ else forced to script or do individually\n",
    "# This is probably when I would lose anyone I had assured it was easy...\n",
    "\n",
    "# Protoc Downoload\n",
    "# https://github.com/protocolbuffers/protobuf/releases/tag/v3.6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PYTHONPATH\n",
    "This can be setup permanently using bash file or windows environment.  You cand just run the following in each terminal session:\n",
    "\n",
    "Linux/Mac - cd to the 'annomator' folder\n",
    "export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/tf_slim_obj_det\n",
    "Windows - no need to cd.  Just change absolute path to your tf_slim_obj_det folder\n",
    "set PYTHONPATH=%PYTHONPATH%;C:\\path_to\\tf_slim_obj_det\n",
    "\n",
    "Note: Does note work from within Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step instructions for Annomator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Notes\n",
    "\n",
    "The simple terminal way:\n",
    "pip install -r annotator_requirements.txt\n",
    "pip install tensorflow==1.5.0\n",
    "pip install protobuf==3.6.1\n",
    "\n",
    "\n",
    "# Simple cross-platform folder structure\n",
    "desktop is ok\n",
    "--tensorflow\n",
    "----tf_150\n",
    "----tf_150_gpu\n",
    "----annomator\n",
    "----coco2017\n",
    "----tf_zoo\n",
    "\n",
    "\n",
    "# Install/update Python (Python 3.6.6 not Python 3.7) - Python 2.7 and 3.3+ ok\n",
    "python -m pip install --upgrade pip # (pip 18) - pip 10+ ok\n",
    "\n",
    "# Create/activate virtual environment\n",
    "pip install virtualenv\n",
    "virtualenv tf_150 (option to include existing Python packages --system-site-packages)\n",
    "cd tf_150\n",
    "Linux/Mac\n",
    "source ./bin/activate\n",
    "Windows\n",
    "cd tf_150\n",
    "activate\n",
    "\n",
    "\n",
    "pip install jupyter\n",
    "pip install pillow==5.0.0\n",
    "pip install matplotlib\n",
    "\n",
    "\n",
    "# Install Tensorflow\n",
    "# Due to potential bleach conflict with jupyter, install\n",
    "pip install tensorflow==1.5.0 # or tensorflow-gpu==1.5.0\n",
    "\n",
    "pip install protobuf==3.6.1\n",
    "\n",
    "# protoc - see Setup.ipynb for details\n",
    "# It may already be the case but this will ensure version control when 3.6.1 is no longer current\n",
    "\n",
    "\n",
    "# Windows 10 \n",
    "# Protoc has already been done for Windows using protobuf 3.6.1 (current as of Oct 2018).  \n",
    "# There are still no official instructions that work and it was easily the most problematic install.  To overwrite the protoc is also much easier for Mac / Linux with a single line.  \n",
    " \n",
    "https://github.com/protocolbuffers/protobuf/releases/tag/v3.6.1\n",
    "\n",
    "# Linux/Mac\n",
    "cd to the tf_slim_obj_det folder\n",
    "\n",
    "\n",
    "See EdjeElectronics for a great tutorial with TF 1.10\n",
    "The only real points of difference is I have stuck with TF 1.5.0 for now.  I don't use the packaged setup or install.\n",
    "\n",
    "Clone or download annomator from github\n",
    "\n",
    "PYTHONPATH if training\n",
    "\n",
    "Linux/Mac - cd to the 'annomator' folder\n",
    "export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/tf_slim_obj_det\n",
    "Windows - no need to cd.  Just change absolute path to your tf_slim_obj_det folder\n",
    "set PYTHONPATH=%PYTHONPATH%;C:\\path_to\\tf_slim_obj_det\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The only thing necessary for the triumph of evil is for good [people] to do nothing - Edmund Burke\n",
    "\n",
    "\n",
    "# Install Notes\n",
    "\n",
    "# You can simply follow the official install for TF OD\n",
    "# I have included a few files I hope will make it easier for some\n",
    "\n",
    "\n",
    "# With the new release I have decided to release my code based on TF 1.5.0 and TF OD 1.0 and TF Slim\n",
    "# I would also rather just click to install and start adding images.  This is a work in progress.\n",
    "\n",
    "# Compatible with 1.5.0 is it is still the only version that will work reliably for everyone.\n",
    "# This version works with Windows, Mac and Linux - AVX or not, Python 2 and 3\n",
    "# This does not lock you into yesteryear.  In fact, Kagglers may find it useful too.\n",
    "# Compatible with TF 1.8.0 models and TF 1.10 so can use with current version (No 1.11 testing)\n",
    "# Get some heat in your 1080 with a cutting edge nasnet grinder within minutes of release\n",
    "# Or make a mini-cloud with any old computer gathering dust\n",
    "\n",
    "This also means you can also afford to research even if you can't afford or reach cloud computing\n",
    "# Analyse years of gathered pictures and get the data in a format you can use\n",
    "\n",
    "# I have also setup trainers for masks or boxes\n",
    "\n",
    "# I have had to develop a series of codecs and end-user tools for this process.  I am happy it can do everything I wanted it to do but it has reached a critical mass of complexity for most.  I am hoping to delete a few things before I add any more so I have released the code for discussion and use.  I am reluctantly launching Annomator 1.0 beta.  I will move to 1.0 release scheduled for 1-11-2018.  and then to 1.0 release when I have the test results back from the beta testers.  \n",
    "\n",
    "I am releasing the codecs and the process in the hope it will help some and ultimately aid research.  I am really looking for bug reports and feedback from experienced users.  \n",
    "\n",
    "\n",
    "\n",
    "# tf_slim_obj_det\n",
    "# This is an attempt at a transparent 20mb of Tensorflow Object Detection API v1.0\n",
    "# It is simply TF Slim folder (without root files) with a copy of object_detection folder inside\n",
    "# The folder structure allows a single path and all native paths work with original code\n",
    "# I have only added 6 characters to 1 line to make it compatible with Python 2.7 and 3.3+\n",
    "# I have only removed the 100mb test graph but the rest is identical and current until Sep 2018\n",
    "\n",
    "# I have tried to make the protoc process a little easier too by including the 3.6.1 binaries\n",
    "# You may have to use pip install protobuf==3.6.1 first to ensure version control\n",
    "# The process is hardest for Windows 10 so it comes setup with protoc completed already (3.6.1)\n",
    "\n",
    "# Linux/Mac simply have to use the following line\n",
    "# If you want to use the protoc binaries in tf_slim_obj_det\n",
    "# protobuf==3.6.1 (This is version control so the existing protoc binaries work)\n",
    "# You can simply use protoc for linux/mac\n",
    "\n",
    "# I have removed the need for pycocotools by concentrating on png masks.  I have included json but not rle or xml. \n",
    "\n",
    "\n",
    "If your name is Ian Frazer, Kingsford Smith you get a prize.  If your name is Rob Whyte or Sheldon Naive you get a prize.  There are countless people who have gone above and beyond in the search for truth.  Unfortunately all I have to offer is some assistance if you need it.  If you are researching or working in Australian Species ID or biomedical research I may be able to help you get you started.  \n",
    "\n",
    "Birds to blood cells, using boxes or masks, allows accurate categorisation and counts from pictures.  This version comes setup ready to annotate and train for beginners, non-coders and researchers.  I have not left students and kagglers out with the ability to drop in the latest model in the time it takes to download and unzip.  Many may prefer to stick to a pure Keras solution but this may also allow people to learn\n",
    "\n",
    ", this project started as AI competition code for Kaggle and can easily be modified to run any model in the time it takes to download and unzip.  Five minutes later any old computer can be crunching on Tensorflow sugar.  \n",
    "\n",
    "way to snap up the cutting edge models released in the treasure trove that is tf model zoo.\n",
    "\n",
    "Please try google and stackoverflow first.  \n",
    "\n",
    "If you submit a complete bug report via github, and a solution, I will be happy to read it.  If there is community concensus, I will be happy to impliment it.  \n",
    "\n",
    "annomator@gmail.com \n",
    "--- do some good --- just some\n",
    "\n",
    "--- skynet is already here.  pick a side.  mynet ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know what you are doing you can install with:\n",
    "Activate virtual environment\n",
    "pip install annotator_requirements.txt\n",
    "pip install tensorflow==1.5.0\n",
    "\n",
    "If you are new to Tensorflow, stick to the cpu version.  If you have Cuda 9 setup you can use\n",
    "pip install tensorflow-gpu==1.5.0\n",
    "Works with 1.5.0 and 1.8.0 models using TF 1.5.0.  \n",
    "Works with 1.5.0 to but version 1.10 tested.  There are still some OS issues that mean building from source is the only option for some.  Until there are binaries that work for TF 1.10 on all platforms I will support 1.5.0.  If you can't wait for my port, 1.10 is and the latest TF OD release is fairly straight forward.  (1.11 is still a bit too new).  Simply move the train.py back out of the object_detection/legacy folder and the current release version should work with any version released version of Tensorflow you can get running.  Stick to Python 3.6.6 as 3.7 too new.  \n",
    "\n",
    "download annomator from github\n",
    "\n",
    "I have included a version of TF Slim and TF Object Detection API V 1.0 as it is still buried in models/research/ amongst countless other projects so it was not easy to find.  It is 19mb and is no longer the current version as of October 2018 so explaining a prior commit to github was just one more step that made it difficult.  To avoid trying to direct beginners through that I have included a relatively complete and unaltered version.   \n",
    "\n",
    "There are plenty of instructions on how to setup Windows Environment or bash files to avoid this step but this should get you started.  Essentially, if you want to train you will need set the PYTHONPATH for every terminal session.  If appropriate, activate your virtual environment, and run the following:\n",
    "\n",
    "Linux/Mac - cd to the 'annomator' folder\n",
    "export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/tf_slim_obj_det\n",
    "\n",
    "Windows - no need to cd.  Just change absolute path to your tf_slim_obj_det folder\n",
    "set PYTHONPATH=%PYTHONPATH%;C:\\path_to\\tf_slim_obj_det\n",
    "\n",
    "If you want to train you will need to run protoc.  The official install instructs to download.  The bin should be copied to the bin folder of virtual env (or system), or Scripts folder on Windows.  You should also copy the lib to your respective lib but I found a copy of the binary is generally enough after install of Protobuf of the same version with Tensorflow.  \n",
    "If you are lucky enough to be running Win10 and Protobuf 3.6.1 you win a prize.  The prize is that you don't need to compile anything.  It can defeat a beginner.  Simply unzip the file called win10_361_protocs.zip into the protocs folder. The other prizes are the binary files for windows, mac and linux are already in the folder tf_slim_obj_det.  You may be able to simply run protoc_lin_361, protoc_win_361 or protoc_mac_361.  The syntax is below.\n",
    " \n",
    "Pycocotools can be used but is not necessary for any codecs, thus removing the other necessary compile.  The TF OD and Coco API install on linux is easy on linux, particularly with a modern computer, a 1080 GPU or a decent net connection with a bank of cloud time.  It is relatively easy on Mac but no binary GPU support since TF 1.0.  It can be difficult on Windows even with all the best hardware.  I am no hater or lover of Windows.  It is mostly sympathy as I had the most difficulty in setups and testing.  It also happens to be the most common OS in any office or home accross the world so it seems Tensorflow 'for everyone' still does not include install instructions that work for Windows.  It is no fault of Google, Tensorflow or Object Detection.  TF 1.5.0 does not require AVX so can even use an old Win7 32-bit with 2GB+ RAM should \n",
    "\n",
    "If you get stuck, the following guide is detailed instructions for all Linux, Windows and Mac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I always code using USA spelling.  Many of my notes may contain metric and Euro/Aust English.  I will correct any code errors to US but I'll give you my metric when you pry it from my cold, dead hands.\n",
    "TF Tensorflow\n",
    "OD Object Detection\n",
    "AI Artificial Inteligence\n",
    "\n",
    "First thing is to say I did not train, build or contribute to Tensorflow code in any way.  It is an amazing bit of technology that Google has gifted and many have worked hard to refine.  The actual list and links are as follows.  I am sure I have missed some.\n",
    "First.  To the individuals that still maintain parts on github.\n",
    "Phil Ferriere for fixing and maintaining Windows COCO API\n",
    "Alexander Kirillov for making and maintaining Panoptic COCO API\n",
    "Edje Electronics for a great introduction with video and appendix and up to date with Win10 TF 1.10\n",
    "Dat Tran for Racoon Detector and answering every last question\n",
    "There are more individuals that have contributed \n",
    "\n",
    "Francois Chollet as he made TF simpler, more powerful and easier to use.  \n",
    "\n",
    "\n",
    "All the teams at Tensorflow, Tensorflow Slim and Tensorflow Object Detection\n",
    "\n",
    "Waleed Abdulla also has a great masker for resnet101 and resnet50 with Keras and Python 3.  It does not use TF OD and is not related at all to this project.  It includes a good overview, working examples and a fully functional keras trainer.  I am going to quote him in response to the TF OD API and if it is now redundant on June 1, 2018.  \"Things change so fast in this field so I don't know how relavant this model will be in the long term.  In the short term (edit), though, I will continue maintaining it.  TF OD is (edit) \"unnecessarily complicated such that it is really hard to understand\".  One year after release and it is just as relevant.  Nice work.\n",
    "\n",
    "I have looked at the release of the 1.8.0 models and TF 1.11 and I have reverted back to the 1.5.0 code I have developed with the previous release TF OD API 1.0 (Feb-Sep 2018).  \n",
    "\n",
    "I think the technology is great but it is not as easy as clicking an install and adding pictures.  I don't expect Google to release an AI Object Detection for researchers, students, noobs and old computers.  I do want to help researchers, students etc get access.  I did witness the cure for cervical cancer go from an idea to fruition.  They would still be mapping the human genome if they hadn't figured out a way to automate the process.  I have witnessed so much research that could benefit, from counting cancer cells, to identifying Australian insects.  27 new species of spider were just discovered last month within 100km of where I live.  We can't respect or respond to what we don't understand and there is so much to understand.  \n",
    "\n",
    "We finally have a bird count from Kingsford smith to text.  It is the best indicator of the health of the Murray Darling System which traverses the Eastern States and has been highly topical for over 10 years.  It is poised to spend 5 billion to help farmers and the environment but data to make the decisions is only for the brave efforts of all the people behind him.  After travelling the entire length in a light aircraft he might appreciate that I can do it from space for flow, detection, dam storage, a drone for species id and handheld for local use.  No more needing to keep recording an auction-like pace audio to be annotated.  If you insist I can just auto-annotate the audio but I would not want to do that poor phd student out of that job...  Hold on.  That is exactly the job I hope skynet takes away.  They should be developing their own research or \n",
    "\n",
    "My personal mission is less ambituous. Environmental management, precision farming, \n",
    "\n",
    "I am grateful for all the models released on TF Zoo and have lot of people to thank.  \n",
    "\n",
    "\n",
    "One way to honour the open source project is to take work, not create it; so I have.\n",
    "\n",
    "The first part is that a server ready, linux, cloud version is not approachable.  \n",
    "Take a sample of experts on Kaggle and see if anyone uses it for competition or professionally.\n",
    "Take a sample of first year software engineers and see if they can get it running on their pc.\n",
    "The chances of an unrelated researcher being able to get it running are unlikely without assistance. \n",
    "I have no stats but I do know keras and matterport dominates for mask rcnn and only has res50 and res101.  Inception V2 Mask RCNN runs 5 to 10 times faster for a drop in pixel accuracy of ~35% to ~25%.  ie It can run 10 times faster for a possible drop in pixel accuracy of 10%?  It is not that simple but yes.  It is far more appropriate for many cases where count of species or cells can be just as good if accuracy score is adjusted.  On the other hand, identifying a face in a crowd accurately reliably may require the most accurate, cutting edge Nasnet.  The accuracy of 43% more relavant to the other models.  It detects very well but will may take 10 times longer.  \n",
    "\n",
    "I have avoided using actual times but unless you have a 1080 GPU don't expect anything close to the ms of 600x600 pics from website.  You could expect a Single Shot Detector to detect large objects at around 1024x1024 in a second.  A Nasnet could take 10 mins (600 times longer).  A GPU will speed up inference and training by 2 to 100 times depending on your system.  10 times faster is average\n",
    "\n",
    "I personally can't recommend using mobilenets unless the output is an android device.  Even then I would prefer a 3 second identification that is correct, than a 300ms guess.  They do run around 10x faster than inception on device but I have found them harder to train and less accurate.  My short list is SSD Inc2 (used for box trainer) Mask Inc2 (used for mask trainer).  Any coco frozen graph can be used with the annotator when first setup.  Other models are simple enough but the project has already reached a level of complexity where testing and feedback from the wider community is required.  \n",
    "\n",
    "This also drops the GPU requirement from around 8 gig to 2 gig.  The cost of the GPU goes from $500US $1000AU to $50 making it more accessible.  My whole quad core custom system runs at around 20 watts at full tilt and will train a new model to a usable level in around 8 hours and done in a day.  A 1080 system can draw 400 watts.  This would not matter except bitcoin uses the same cuda based gpu.  Efficiency at scale does matter but total cost of hardware $400US ($550AU) including gpu.  Total power cost is 15cents at 30c/Kwh for a day of training.  Depreciation etc aside, sending 4k video is simply not practical let alone efficient.  \n",
    "I have aimed at running on most computers - Linux, Windows and Mac running Python 2.7 and 3.3-3.6.6\n",
    " The goal was to make it easy enough for a researcher from an unrelated field is able to simply annotate from detections.  This would allow for countless research projects to leap forward at little cost.  \n",
    "\n",
    "\n",
    "# The last version would fail to train in Python 3 on most platforms and the latest version insists on python 3 for windows...\n",
    "# I have made sure it works on most platforms without compiling from source\n",
    "\n",
    "# There is no other os agnostic code to get any old computer setup and running the latest cutting edge models within minutes.\n",
    "\n",
    "# Install is simple for demo/annotator/detection and currently comes setup for any coco model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "# 1 Check Python and pip\n",
    " Open a terminal - Google for your Computer\n",
    "python -V (or python --version)\n",
    "It is compatible with Python 2.7 if you but I would suggest 3.6.6 as latest (not 3.7 yet)\n",
    "\n",
    "Install Python 3.3 to 3.6.6 (latest below 3.7)\n",
    "Everything works with python 2.7 also\n",
    "\n",
    " Ensure pip is up to date\n",
    " Display what is already present and possibly an update\n",
    "pip list \n",
    " You are using pip version ###, however version ### is available.\n",
    " You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
    "! python -m pip install --upgrade pip\n",
    "\n",
    "# 2 Setup virtual environment\n",
    "pip install virtualenv\n",
    " To make a new virtual environment and install all current python packages\n",
    " cd to the location you would like your working folder\n",
    "virtualenv --system-site-packages annomator\n",
    " To just make a new virtual environment with python and pip\n",
    " virtualenv tf_150\n",
    "\n",
    " 3 Activate the environment\n",
    " You should find a folder of 'environment name' created\n",
    " cd into the folder - Linux/Mac should find find a bin folder and Windows a Scripts folder\n",
    " Linux/Mac is can be activated with 'source ./bin/acitvate'\n",
    " Windows can be activated with 'cd Scripts' and then 'activate'\n",
    " Note use 'deactivate' to close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The annotator requirements install jupyter, which you obviously have running\n",
    "# More detailed instructions are in Setup.ipynb\n",
    "# It is recommended to use Python 3 and virtualenv\n",
    "# Note for Native/System install use sudo pip for Linux/Mac or 'run as administrator' for Windows\n",
    "# Versions that work on most Linux, Windows and Apple Macs\n",
    "# Virtual Environment/Conda: pip install -r annotator_requirements.txt\n",
    "jupyter\n",
    "pillow==5.0.0\n",
    "matplotlib\n",
    "\n",
    "# Then install Tensorflow or Tensorflow GPU (Cuda 9)\n",
    "# pip install tensorflow==1.5.0\n",
    "# Note - GPU requires Cuda 9, Nvidia GPU compute capability 3.5+ etc\n",
    "# pip install tensorflow-gpu==1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now just copy the annomator from github\n",
    "# The annotators will work with no protoc etc\n",
    "# The setup folder is helpful but it only requires anno_repo and frozen_graph to run\n",
    "\n",
    "# To train, you will also need the tfsod folder, train boxes and/or train masks\n",
    "# I have written the tf record creator but the rest is largely unchanged\n",
    "\n",
    "# I have packaged the mask trainer with a COCO Inception V2 Mask RCNN - The fastest masker\n",
    "# I have packaged the box trainer with a COCO Inception V2 SSD\n",
    "\n",
    "# Works with all MSCOCO Models and more are available. The project already involves everything from image resizers to mask dict caches.\n",
    "# If the community is interested I can release codecs for more \n",
    "\n",
    "# Includes output options of png, jpg, pdf for images, txt, csv or json for text files, or png images.\n",
    "# pdf, jpg and txt, csv for home/office level user on any device\n",
    "# json boxes is coded for ease of use and change\n",
    "# The png images have a number of codecs to choose from including simple binary to color encoded condonsed images.\n",
    "# Several are presented for the first time here:\n",
    "# The simple raw, offset can be used for simple datasets\n",
    "# The colors can be subtle shades of gray, natural, unique or distinct\n",
    "# The category, category count and instance count is also be embedded in every codec\n",
    "\n",
    "# This has many options but at the core are some design principles and mission objectives:\n",
    "# Concentrate on running locally and not server and cloud setup\n",
    "# auto-downloading disabled so download multiple GB models\n",
    "# Remove argparsing and use common python where possible - simplifies\n",
    "# Ensure works for gpu or not, Python 2 or 3 and Linux, Mac and Windows\n",
    "# Remove the need to install compiler, update, make, setup, install or protoc for demo/annotate\n",
    "# Only protoc needed for training as uses pipeline, pbtxt for full compatibility with TFOD\n",
    "# No rle or xml - json is only needed if you want to use it\n",
    "# You can annotate and train boxes or masks using condensed png of 6 different types\n",
    "# Use your own data and annotate by hand too.  Annotate, edit, train and back to annotate\n",
    "# Not needing pycocotools avoids Visual Studio etc which can all be problematic for Windows\n",
    "# Only used for evaluation so not used - Tensorboard all works and can use pycocotools if you want\n",
    "\n",
    "# Use any coco box or mask model.  All code is compatible with 1.5.0 and 1.8.0 models\n",
    "# Small alterations needed for other datasets - More models and codecs coming soon\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You also need to get the latest version of protoc for your computer\n",
    "# The file is small but so are the setup instructions\n",
    "# You can copy the bin to the bin and the lib to the lib on any platform\n",
    "# I found that just the protos.exe in bin (linux/mac) Scripts (win) or the tf_slim_obj_det folder\n",
    "\n",
    "cd to the tf_slim_obj_det\n",
    "# If you are on Linux/Mac you can simply use\n",
    "# ref\n",
    "# If you are on Windows you can copy and paste the following\n",
    "protoc --python_out=. .\\object_detection\\protos\\anchor_generator.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\argmax_matcher.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\bipartite_matcher.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\box_coder.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\box_predictor.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\eval.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\faster_rcnn.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\faster_rcnn_box_coder.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\graph_rewriter.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\grid_anchor_generator.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\hyperparams.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\image_resizer.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\input_reader.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\keypoint_box_coder.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\losses.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\matcher.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\mean_stddev_box_coder.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\model.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\multiscale_anchor_generator.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\optimizer.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\pipeline.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\post_processing.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\preprocessor.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\region_similarity_calculator.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\square_box_coder.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\ssd.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\ssd_anchor_generator.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\string_int_label_map.proto\n",
    "protoc --python_out=. .\\object_detection\\protos\\train.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one more step that needs to be done when using any of the training scripts\n",
    "# There are instructions for Linux, Mac and Windows to change environment variables or bash files etc\n",
    "# It must otherwise be done in every terminal session \n",
    "\n",
    "# Linux/Mac - cd to the 'annomator' root folder\n",
    "export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/tf_slim_obj_det\n",
    "# Windows - change path to tf_slim_obj_det\n",
    "set PYTHONPATH=%PYTHONPATH%;C:\\path_to\\tf_slim_obj_det\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more ways to get lost along the way.  Please assume that the google gods are busy and that I am already directing my 'spare time' to helping the most riteous researchers.  You should not expect a reply if you have not at least googled it.  Try stackoverflow.  \n",
    "\n",
    "If you are working in research, environmental management, freshwater ecology I may be able to give you the answer to an ungooglable question.  I am currently working on Australian Species ID applications and will continue to prioritise medical research.  That said I will endeavor to intercept the tears and yelling that normally happens if you simply follow the install instructions.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to edit or annotate masks you will also need an image program.  Basic Paint programs will often scramble (with anti-aliasing) a mask so use Gimp with anti-aliasing off.  Gimp is a free, cross-platform image program and is my current recomendation to annotate or edit png masks.  \n",
    "\n",
    "The current version can use json or just color a rough outline and train from the box (or mask) it would make.  This avoids the need for any external box annotator or any external xml or json.  \n",
    "\n",
    "The last thing to download is possibly the MSCOCO Panoptic data.  I have built a json box trainer that uses the panoptic format.  I have also built a converter based on the panoptic png images.  If you want use coco for testing or training, all options are possible using the 2017 dataset.  Download the 5000 val images for a play with the codecs or 120 000 training that include all 80 'Things' common in standard mscoco and 'Stuff'/texture/background classes of water, sand, sky etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I am also a fan of xnconvert and ffmpeg for batched image conversions, VLC and Blender for video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also have color charts for easy annotations and other models lined up for release.  I also have codecs for 255+ categories but the project has already reached a level of complexity that I think needs to be reduced and tested before adding more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotator\n",
    "##### Installation instructions for the annotator\n",
    "The annotator should work with any tensorflow 1.5+.  It only requires Pillow to read images and matplotlib to display.  No compiling, protoc, coco api, pycocotools, rle required.  You don't even need json unless you want to annotate or train boxes.\n",
    "\n",
    "For Office - pdf or jpg, txt or csv\n",
    "Boxes for Beginners - png or json with simple code\n",
    "Masks for Masters - png condensed \n",
    "\n",
    "A separate mscoco category index is included so no protoc needed for pbtxt\n",
    "Comes setup with Inception V2 Mask RCNN setup and ready to detect masks and boxes\n",
    "Works with TF 1.5.0 to 1.10 without custom install\n",
    "Choices of outputs are png, pdf, jpg, txt, csv, json, condensed pngs or binary pngs\n",
    "All are compatible with Tensorflow Object Detection 1.5.0 and 1.8.0\n",
    "All are compatible with any Tensorflow Zoo Model (Setup for mscoco including panoptic)\n",
    "All are compatible with Python 2.7 to Python 3.7\n",
    "All instructions are given for Linux, Windows and Mac\n",
    "\n",
    "Box and Mask annnotations can then be used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tips\n",
    "Use Tensorflow 1.5.0 for older computers (No AVX means 'Illegal Operation - memory dumped')\n",
    "\n",
    "Ignore the official installation instructions\n",
    "Do use a virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Trainer and Mask Trainer\n",
    "##### Installation instructions for theTrainers\n",
    "\n",
    "##### Tensorflow Object Detection API\n",
    "You are more than welcome to install using the official instructions.\n",
    "Either the earlier 1.5.0 models or 1.8.0 models will work.\n",
    "\n",
    "I had high hopes for the new 1.8.0 models and 1.9+ setup requirements but I have reverted to using the previous release with a few minor modifications.  Included is a copy of TF Slim and Object Detection (prior release)\n",
    "\n",
    "The changes are as follows:\n",
    "I moved a copy of object detection into the slim folder and renamed to tf_slim_obj_det\n",
    "I removed root files except __init__.py to make it simpler\n",
    "I removed the ~100mb ssd model from test_ckpt\n",
    "I fixed train.py to work with Python 3\n",
    "Removed argparsing\n",
    "Leaves ~20mb\n",
    "\n",
    "You do not need to install, setup or make TF Slim, TF Object Detection API or COCO API\n",
    "You still need to download the protoc and run\n",
    "You still need to use PYTHONPATH but only to tf_slim_obj_det\n",
    "Linux/Mac\n",
    "export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/tf_slim_obj_det\n",
    "Windows\n",
    "set PYTHONPATH=%PYTHONPATH%;C:\\path_to\\tf_slim_obj_det\n",
    "\n",
    "##### I have written TF Record creators for boxes and masks\n",
    "You can train from coco panoptic, auto annotations or encode images by hand.\n",
    "You can train from json, png condensed or binary images.\n",
    "No RLE, xml or pycocotools needed - json optional\n",
    "\n",
    "I have then handed over to the train file in tf_slim_obj_det.  Use the standard pipeline and checkpoint.  Comes with Inception V2 Mask RCNN setup but works with all current models on TF Zoo with minor modifications.\n",
    "\n",
    "See the Train Boxes and Train Masks notebooks for a walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First thing is to check what version of python you have\n",
    "! python --version\n",
    "! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow 1.5.0\n",
      "Cython 0.28.3\n",
      "Matplotlib 2.2.2\n",
      "PIL or pillow 5.0.0\n"
     ]
    }
   ],
   "source": [
    "# You can check them off yourself but will try importing requirements\n",
    "import tensorflow\n",
    "print(\"Tensorflow\", tensorflow.__version__)\n",
    "import Cython\n",
    "print(\"Cython\", Cython.__version__)\n",
    "import matplotlib\n",
    "print(\"Matplotlib\", matplotlib.__version__)\n",
    "import PIL\n",
    "print(\"PIL or pillow\", PIL.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
